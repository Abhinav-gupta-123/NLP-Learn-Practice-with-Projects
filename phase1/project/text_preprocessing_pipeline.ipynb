{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (2.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (78.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\abhin\\desktop\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - ------------------------------------- 0.5/12.8 MB 430.4 kB/s eta 0:00:29\n",
      "     - ------------------------------------- 0.5/12.8 MB 430.4 kB/s eta 0:00:29\n",
      "     - ------------------------------------- 0.5/12.8 MB 430.4 kB/s eta 0:00:29\n",
      "     -- ------------------------------------ 0.8/12.8 MB 435.8 kB/s eta 0:00:28\n",
      "     -- ------------------------------------ 0.8/12.8 MB 435.8 kB/s eta 0:00:28\n",
      "     --- ----------------------------------- 1.0/12.8 MB 430.2 kB/s eta 0:00:28\n",
      "     --- ----------------------------------- 1.0/12.8 MB 430.2 kB/s eta 0:00:28\n",
      "     --- ----------------------------------- 1.0/12.8 MB 430.2 kB/s eta 0:00:28\n",
      "     --- ----------------------------------- 1.3/12.8 MB 432.9 kB/s eta 0:00:27\n",
      "     --- ----------------------------------- 1.3/12.8 MB 432.9 kB/s eta 0:00:27\n",
      "     --- ----------------------------------- 1.3/12.8 MB 432.9 kB/s eta 0:00:27\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 441.5 kB/s eta 0:00:26\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 441.5 kB/s eta 0:00:26\n",
      "     ----- --------------------------------- 1.8/12.8 MB 453.4 kB/s eta 0:00:25\n",
      "     ----- --------------------------------- 1.8/12.8 MB 453.4 kB/s eta 0:00:25\n",
      "     ------ -------------------------------- 2.1/12.8 MB 462.3 kB/s eta 0:00:24\n",
      "     ------ -------------------------------- 2.1/12.8 MB 462.3 kB/s eta 0:00:24\n",
      "     ------ -------------------------------- 2.1/12.8 MB 462.3 kB/s eta 0:00:24\n",
      "     ------- ------------------------------- 2.4/12.8 MB 464.4 kB/s eta 0:00:23\n",
      "     ------- ------------------------------- 2.4/12.8 MB 464.4 kB/s eta 0:00:23\n",
      "     ------- ------------------------------- 2.6/12.8 MB 469.0 kB/s eta 0:00:22\n",
      "     ------- ------------------------------- 2.6/12.8 MB 469.0 kB/s eta 0:00:22\n",
      "     -------- ------------------------------ 2.9/12.8 MB 478.0 kB/s eta 0:00:21\n",
      "     -------- ------------------------------ 2.9/12.8 MB 478.0 kB/s eta 0:00:21\n",
      "     --------- ----------------------------- 3.1/12.8 MB 488.2 kB/s eta 0:00:20\n",
      "     --------- ----------------------------- 3.1/12.8 MB 488.2 kB/s eta 0:00:20\n",
      "     ----------- --------------------------- 3.7/12.8 MB 538.5 kB/s eta 0:00:17\n",
      "     ----------- --------------------------- 3.9/12.8 MB 568.7 kB/s eta 0:00:16\n",
      "     ----------- --------------------------- 3.9/12.8 MB 568.7 kB/s eta 0:00:16\n",
      "     ------------ -------------------------- 4.2/12.8 MB 574.5 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 574.5 kB/s eta 0:00:15\n",
      "     ------------- ------------------------- 4.5/12.8 MB 582.3 kB/s eta 0:00:15\n",
      "     ------------- ------------------------- 4.5/12.8 MB 582.3 kB/s eta 0:00:15\n",
      "     ------------- ------------------------- 4.5/12.8 MB 582.3 kB/s eta 0:00:15\n",
      "     ------------- ------------------------- 4.5/12.8 MB 582.3 kB/s eta 0:00:15\n",
      "     ------------- ------------------------- 4.5/12.8 MB 582.3 kB/s eta 0:00:15\n",
      "     ------------- ------------------------- 4.5/12.8 MB 582.3 kB/s eta 0:00:15\n",
      "     -------------- ------------------------ 4.7/12.8 MB 514.8 kB/s eta 0:00:16\n",
      "     -------------- ------------------------ 4.7/12.8 MB 514.8 kB/s eta 0:00:16\n",
      "     -------------- ------------------------ 4.7/12.8 MB 514.8 kB/s eta 0:00:16\n",
      "     -------------- ------------------------ 4.7/12.8 MB 514.8 kB/s eta 0:00:16\n",
      "     -------------- ------------------------ 4.7/12.8 MB 514.8 kB/s eta 0:00:16\n",
      "     -------------- ------------------------ 4.7/12.8 MB 514.8 kB/s eta 0:00:16\n",
      "     -------------- ------------------------ 4.7/12.8 MB 514.8 kB/s eta 0:00:16\n",
      "     -------------- ------------------------ 4.7/12.8 MB 514.8 kB/s eta 0:00:16\n",
      "     --------------- ----------------------- 5.0/12.8 MB 463.9 kB/s eta 0:00:17\n",
      "     --------------- ----------------------- 5.0/12.8 MB 463.9 kB/s eta 0:00:17\n",
      "     --------------- ----------------------- 5.0/12.8 MB 463.9 kB/s eta 0:00:17\n",
      "     --------------- ----------------------- 5.0/12.8 MB 463.9 kB/s eta 0:00:17\n",
      "     --------------- ----------------------- 5.0/12.8 MB 463.9 kB/s eta 0:00:17\n",
      "     --------------- ----------------------- 5.0/12.8 MB 463.9 kB/s eta 0:00:17\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     --------------- ----------------------- 5.2/12.8 MB 428.4 kB/s eta 0:00:18\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 365.5 kB/s eta 0:00:20\n",
      "     ----------------- --------------------- 5.8/12.8 MB 317.1 kB/s eta 0:00:23\n",
      "     ----------------- --------------------- 5.8/12.8 MB 317.1 kB/s eta 0:00:23\n",
      "     ------------------ -------------------- 6.0/12.8 MB 323.5 kB/s eta 0:00:21\n",
      "     ------------------ -------------------- 6.0/12.8 MB 323.5 kB/s eta 0:00:21\n",
      "     ------------------- ------------------- 6.3/12.8 MB 331.8 kB/s eta 0:00:20\n",
      "     ------------------- ------------------- 6.6/12.8 MB 341.2 kB/s eta 0:00:19\n",
      "     -------------------- ------------------ 6.8/12.8 MB 351.0 kB/s eta 0:00:18\n",
      "     --------------------- ----------------- 7.1/12.8 MB 360.8 kB/s eta 0:00:16\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 370.7 kB/s eta 0:00:15\n",
      "     ----------------------- --------------- 7.6/12.8 MB 380.4 kB/s eta 0:00:14\n",
      "     ----------------------- --------------- 7.9/12.8 MB 390.8 kB/s eta 0:00:13\n",
      "     ------------------------ -------------- 8.1/12.8 MB 400.1 kB/s eta 0:00:12\n",
      "     ------------------------- ------------- 8.4/12.8 MB 408.9 kB/s eta 0:00:11\n",
      "     -------------------------- ------------ 8.7/12.8 MB 417.2 kB/s eta 0:00:10\n",
      "     --------------------------- ----------- 8.9/12.8 MB 424.9 kB/s eta 0:00:10\n",
      "     --------------------------- ----------- 9.2/12.8 MB 433.4 kB/s eta 0:00:09\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 441.8 kB/s eta 0:00:08\n",
      "     ----------------------------- --------- 9.7/12.8 MB 450.4 kB/s eta 0:00:07\n",
      "     ------------------------------ ------- 10.2/12.8 MB 467.7 kB/s eta 0:00:06\n",
      "     ------------------------------- ------ 10.5/12.8 MB 476.6 kB/s eta 0:00:05\n",
      "     ------------------------------- ------ 10.7/12.8 MB 485.2 kB/s eta 0:00:05\n",
      "     --------------------------------- ---- 11.3/12.8 MB 502.9 kB/s eta 0:00:04\n",
      "     ----------------------------------- -- 11.8/12.8 MB 520.6 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 12.1/12.8 MB 529.4 kB/s eta 0:00:02\n",
      "     -------------------------------------  12.6/12.8 MB 546.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 12.8/12.8 MB 553.3 kB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Install spaCy if not already installed\n",
    "!pip install spacy\n",
    "\n",
    "# Download the English language model\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nirmala Sitharaman to equal Morarji Desai’s re...</td>\n",
       "      <td>With the presentation of the interim budget on...</td>\n",
       "      <td>Sitharaman, the first full-time woman finance ...</td>\n",
       "      <td>https://indianexpress.com/article/business/bud...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘Will densify network, want to be at least no....</td>\n",
       "      <td>'In terms of market share, we aim to double it...</td>\n",
       "      <td>The merger of Tata group’s budget airlines Air...</td>\n",
       "      <td>https://indianexpress.com/article/business/avi...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Air India group to induct an aircraft every si...</td>\n",
       "      <td>Air India currently has 117 operational aircra...</td>\n",
       "      <td>The Air India group plans to induct one aircra...</td>\n",
       "      <td>https://indianexpress.com/article/business/avi...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red Sea woes: Exporters seek increased credit ...</td>\n",
       "      <td>Rising attacks forced shippers to consider the...</td>\n",
       "      <td>Indian exporters have asked the central govern...</td>\n",
       "      <td>https://indianexpress.com/article/business/red...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Air India group to induct a plane every 6 days...</td>\n",
       "      <td>Apart from fleet expansion, 2024 will also see...</td>\n",
       "      <td>The Air India group plans to induct one aircra...</td>\n",
       "      <td>https://indianexpress.com/article/business/avi...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  Nirmala Sitharaman to equal Morarji Desai’s re...   \n",
       "1  ‘Will densify network, want to be at least no....   \n",
       "2  Air India group to induct an aircraft every si...   \n",
       "3  Red Sea woes: Exporters seek increased credit ...   \n",
       "4  Air India group to induct a plane every 6 days...   \n",
       "\n",
       "                                         description  \\\n",
       "0  With the presentation of the interim budget on...   \n",
       "1  'In terms of market share, we aim to double it...   \n",
       "2  Air India currently has 117 operational aircra...   \n",
       "3  Rising attacks forced shippers to consider the...   \n",
       "4  Apart from fleet expansion, 2024 will also see...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Sitharaman, the first full-time woman finance ...   \n",
       "1  The merger of Tata group’s budget airlines Air...   \n",
       "2  The Air India group plans to induct one aircra...   \n",
       "3  Indian exporters have asked the central govern...   \n",
       "4  The Air India group plans to induct one aircra...   \n",
       "\n",
       "                                                 url  category  \n",
       "0  https://indianexpress.com/article/business/bud...  business  \n",
       "1  https://indianexpress.com/article/business/avi...  business  \n",
       "2  https://indianexpress.com/article/business/avi...  business  \n",
       "3  https://indianexpress.com/article/business/red...  business  \n",
       "4  https://indianexpress.com/article/business/avi...  business  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "file_path = r\"C:\\Users\\abhin\\Desktop\\NLP\\phase1\\news_articles_datasets\\business_data.csv\"  # Ensure it's in your working directory\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    doc=nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download(\"punkt\") \n",
    " # Ensure NLTK is ready\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\abhin/nltk_data', 'c:\\\\Users\\\\abhin\\\\Desktop\\\\.venv\\\\nltk_data', 'c:\\\\Users\\\\abhin\\\\Desktop\\\\.venv\\\\share\\\\nltk_data', 'c:\\\\Users\\\\abhin\\\\Desktop\\\\.venv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\abhin\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run runner run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_text(text):\n",
    "    # Initialize the Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    # Split the text into words based on whitespace\n",
    "    words = text.split()\n",
    "    # Apply stemming to each word\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # Join the stemmed words back into a single string\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "# Example usage:\n",
    "print(stem_text(\"running runners run\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmetize_text(text):\n",
    "    doc=nlp(text)\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    return pos_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "    return \" \".join(filtered_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"C:/Users/abhin/AppData/Roaming/nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\AppData\\Roaming\\nltk_data\\corpora\\wordnet\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.find(\"corpora/wordnet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"C:/Users/abhin/AppData/Roaming/nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('example.n.01'), Synset('model.n.07'), Synset('exemplar.n.01'), Synset('example.n.04'), Synset('case.n.01'), Synset('exercise.n.04')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "print(wordnet.synsets(\"example\"))  # Should return wordnet results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "print(wordnet.synsets(\"example\"))  # Should return wordnet results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>named_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sitharaman, the first full-time woman finance ...</td>\n",
       "      <td>[Sitharaman, ,, the, first, full, -, time, wom...</td>\n",
       "      <td>sitharaman, the first full-tim woman financ mi...</td>\n",
       "      <td>Sitharaman , the first full - time woman finan...</td>\n",
       "      <td>[(Sitharaman, PROPN), (,, PUNCT), (the, DET), ...</td>\n",
       "      <td>[(Sitharaman, ORG), (first, ORDINAL), (five, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The merger of Tata group’s budget airlines Air...</td>\n",
       "      <td>[The, merger, of, Tata, group, ’s, budget, air...</td>\n",
       "      <td>the merger of tata group’ budget airlin air in...</td>\n",
       "      <td>the merger of Tata group ’s budget airline Air...</td>\n",
       "      <td>[(The, DET), (merger, NOUN), (of, ADP), (Tata,...</td>\n",
       "      <td>[(Tata, ORG), (Air India Express, ORG), (AI Ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Air India group plans to induct one aircra...</td>\n",
       "      <td>[The, Air, India, group, plans, to, induct, on...</td>\n",
       "      <td>the air india group plan to induct one aircraf...</td>\n",
       "      <td>the Air India group plan to induct one aircraf...</td>\n",
       "      <td>[(The, DET), (Air, PROPN), (India, PROPN), (gr...</td>\n",
       "      <td>[(Air India, ORG), (one, CARDINAL), (every six...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Indian exporters have asked the central govern...</td>\n",
       "      <td>[Indian, exporters, have, asked, the, central,...</td>\n",
       "      <td>indian export have ask the central govern to h...</td>\n",
       "      <td>indian exporter have ask the central governmen...</td>\n",
       "      <td>[(Indian, ADJ), (exporters, NOUN), (have, AUX)...</td>\n",
       "      <td>[(Indian, NORP), (nearly 300 per cent, MONEY),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Air India group plans to induct one aircra...</td>\n",
       "      <td>[The, Air, India, group, plans, to, induct, on...</td>\n",
       "      <td>the air india group plan to induct one aircraf...</td>\n",
       "      <td>the Air India group plan to induct one aircraf...</td>\n",
       "      <td>[(The, DET), (Air, PROPN), (India, PROPN), (gr...</td>\n",
       "      <td>[(Air India, ORG), (one, CARDINAL), (every six...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Sitharaman, the first full-time woman finance ...   \n",
       "1  The merger of Tata group’s budget airlines Air...   \n",
       "2  The Air India group plans to induct one aircra...   \n",
       "3  Indian exporters have asked the central govern...   \n",
       "4  The Air India group plans to induct one aircra...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Sitharaman, ,, the, first, full, -, time, wom...   \n",
       "1  [The, merger, of, Tata, group, ’s, budget, air...   \n",
       "2  [The, Air, India, group, plans, to, induct, on...   \n",
       "3  [Indian, exporters, have, asked, the, central,...   \n",
       "4  [The, Air, India, group, plans, to, induct, on...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  sitharaman, the first full-tim woman financ mi...   \n",
       "1  the merger of tata group’ budget airlin air in...   \n",
       "2  the air india group plan to induct one aircraf...   \n",
       "3  indian export have ask the central govern to h...   \n",
       "4  the air india group plan to induct one aircraf...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  Sitharaman , the first full - time woman finan...   \n",
       "1  the merger of Tata group ’s budget airline Air...   \n",
       "2  the Air India group plan to induct one aircraf...   \n",
       "3  indian exporter have ask the central governmen...   \n",
       "4  the Air India group plan to induct one aircraf...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(Sitharaman, PROPN), (,, PUNCT), (the, DET), ...   \n",
       "1  [(The, DET), (merger, NOUN), (of, ADP), (Tata,...   \n",
       "2  [(The, DET), (Air, PROPN), (India, PROPN), (gr...   \n",
       "3  [(Indian, ADJ), (exporters, NOUN), (have, AUX)...   \n",
       "4  [(The, DET), (Air, PROPN), (India, PROPN), (gr...   \n",
       "\n",
       "                                      named_entities  \n",
       "0  [(Sitharaman, ORG), (first, ORDINAL), (five, C...  \n",
       "1  [(Tata, ORG), (Air India Express, ORG), (AI Ex...  \n",
       "2  [(Air India, ORG), (one, CARDINAL), (every six...  \n",
       "3  [(Indian, NORP), (nearly 300 per cent, MONEY),...  \n",
       "4  [(Air India, ORG), (one, CARDINAL), (every six...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply NLP functions to the \"content\" column\n",
    "df[\"tokens\"] = df[\"content\"].astype(str).apply(tokenize_text)\n",
    "df[\"stemmed\"] = df[\"content\"].astype(str).apply(stem_text)\n",
    "df[\"lemmatized\"] = df[\"content\"].astype(str).apply(lemmetize_text)\n",
    "df[\"pos_tags\"] = df[\"content\"].astype(str).apply(pos_tagging)\n",
    "df[\"named_entities\"] = df[\"content\"].astype(str).apply(extract_named_entities)\n",
    "df[\"without stopwords\"] = df[\"content\"].astype(str).apply(remove_stopwords)\n",
    "\n",
    "# Display processed data\n",
    "df[[\"content\", \"tokens\", \"stemmed\", \"lemmatized\", \"pos_tags\", \"named_entities\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NLP Preprocessing Pipeline Complete! Data saved as processed_business_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "df.to_csv(\"processed_business_data.csv\", index=False)\n",
    "print(\"✅ NLP Preprocessing Pipeline Complete! Data saved as processed_business_data.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
